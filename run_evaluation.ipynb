{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3e51e8",
   "metadata": {},
   "source": [
    "# Run NaviTrace Evaluation\n",
    "\n",
    "This notebook describes the process of evaluating models on our benchmark [NaviTrace](https://leggedrobotics.github.io/navitrace_webpage/), including model inference via API and the score calculation.\n",
    "The benchmark consists of a validation split and a test split with hidden ground-truths.\n",
    "If you want to see how your model scores on the test set or want to submit your model to the leaderboard, check out this [Hugging Face Space](https://huggingface.co/spaces/leggedrobotics/navitrace_leaderboard).\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Create and activate a Python 3.10 environment with your preferred tools\n",
    "2. `pip install -r ./requirements.txt`\n",
    "3. Prepare an API key and base URL for the model that you want to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5294cc1",
   "metadata": {},
   "source": [
    "## Load NaviTrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itables import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eefabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Login at HF\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527498bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"leggedrobotics/navitrace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e962fe",
   "metadata": {},
   "source": [
    "Have a look at the [dataset card](https://huggingface.co/datasets/leggedrobotics/navitrace) for information about the available columns. You can also explore the dataset with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset (without images)\n",
    "df = dataset[\"validation\"].to_pandas().drop(columns=[\"image\"])\n",
    "show(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b92c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images\n",
    "img = dataset[\"validation\"][42][\"image\"]\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72889da9",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from PIL.Image import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee94a7a",
   "metadata": {},
   "source": [
    "### Setup Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a741ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "os.environ['MODEL_API_KEY'] = getpass(\"Enter your model API key: \")\n",
    "\n",
    "# Settings\n",
    "# Note: We use OpenRouter to access multiple models but you can also call a provider directly\n",
    "model_name = \"google/gemini-2.5-pro\"\n",
    "base_url = \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b5958",
   "metadata": {},
   "source": [
    "### Define Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a navigation expert for various embodiments including robots and humans. Given an image of the current scenario, a specified embodiment (e.g., legged robot, wheeled robot, human, or bike), and a navigation task (e.g., \"Go down the road\"), you will predict a feasible future trajectory as a sequence of 2D points in normalized image coordinates (ranging from 0 to 1, where [0,0] is the top-left and [1,1] is the bottom-right).\n",
    "\n",
    "- The image shows a first-person view of the navigation scenario\n",
    "- Start your trajectory near the bottom center of the image, which corresponds approximately to normalized coordinate [0.5, 0.95] (representing the current position of the embodiment)\n",
    "- The trajectory should be adapted to the embodiment's abilities and limitations\n",
    "- Plan the path forward from this starting position based on what the embodiment can see and navigate\n",
    "- The trajectory should extend all the way to the goal if the path is visible. If the path is occluded, the trajectory should end where the path becomes fully obscured, unless the path can be reasonably inferred from the visible context.\n",
    "- If a red traffic light is visible and affects the planned path, or if crossing traffic or moving vehicles are present that make it unsafe to proceed, stop at an appropriate waiting position (e.g., just before the intersection or curb) and end the trajectory there.\n",
    "- All tasks that you are given have a solution\n",
    "- Output **only** the list of 2D points in normalized image coordinates (values between 0 and 1) in the following format: `[[x1, y1], [x2, y2], ..., [xn, yn]]`\n",
    "- Do not include any explanation or additional output\n",
    "\n",
    "### Embodiment Movement Characteristics\n",
    "\n",
    "- **Human**: A standard pedestrian. Can navigate stairs and ramps but cannot climb tall obstacles.\n",
    "- **Legged Robot**: A quadruped like ANYmal. Behaves similarly to a human, but it is shorter. It can handle stairs and escalators.\n",
    "- **Wheeled Robot**: A wheeled delivery robot. Behaves like a wheelchair, preferring smooth surfaces such as walkways and ramps. It cannot use stairs or escalators.\n",
    "- **Bicycle**: A standard cyclist. Follows traffic regulations and prefers bike lanes or streets. Cannot navigate stairs.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"**Embodiment**: {embodiment}\n",
    "**Task**: {task}\n",
    "\n",
    "The image shows a first-person view from the embodiment's current position. Begin your trajectory near the bottom center of the image (around normalized coordinate [0.5, 0.95]) and predict the path forward as a list of 2D points in normalized coordinates (values from 0 to 1) according to the embodiment and the scenario shown in the image.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce8947",
   "metadata": {},
   "source": [
    "### Define API Model Class\n",
    "\n",
    "Processing a single sample produces a dict with the form:\n",
    "\n",
    "| Column | Type | Description |\n",
    "| --- | ---- | ----------- |\n",
    "| sample_id | `str` | Unique identifier of a scenario |\n",
    "| embodiment | `str` | Selected embodiment |\n",
    "| category | `List[str]` | Scenario categories |\n",
    "| raw_response | `str` | Raw text response of the model |\n",
    "| reasoning | `str` | If available, the reasoning output of the model |\n",
    "| prediction | `List[List[float]]` | List of [x, y] points representing the predicted trace |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image: Image) -> str:\n",
    "\n",
    "    # Convert to RGB if necessary\n",
    "    if image.mode in (\"RGBA\", \"P\"):\n",
    "        image = image.convert(\"RGB\")\n",
    "    # Save image to a bytes buffer as JPEG\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\")\n",
    "    buffer.seek(0)\n",
    "    # Encode buffer in base64\n",
    "    img_bytes = buffer.read()\n",
    "    img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "    return img_b64\n",
    "\n",
    "def parse_trace(text: str) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Parse point trace from model response.\n",
    "    Expected format: [[x1, y1], [x2, y2], ...] or similar variations.\n",
    "    Returns a list of [x, y] coordinate pairs, or an empty list if parsing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Try to find JSON-like array in response\n",
    "        patterns = [\n",
    "            r\"\\[\\s*\\[[\\d\\s,.-]+\\]\\s*(?:,\\s*\\[[\\d\\s,.-]+\\]\\s*)*\\]\",  # [[x,y], [x,y], ...]\n",
    "            r\"\\(\\s*\\([\\d\\s,.-]+\\)\\s*(?:,\\s*\\([\\d\\s,.-]+\\)\\s*)*\\)\",  # ((x,y), (x,y), ...)\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                # Parse the first match\n",
    "                match = matches[0]\n",
    "                # Convert to proper JSON format\n",
    "                match = match.replace(\"(\", \"[\").replace(\")\", \"]\")\n",
    "                points = json.loads(match)\n",
    "                # Validate format\n",
    "                if all(isinstance(p, list) and len(p) == 2 for p in points):\n",
    "                    return [[float(p[0]), float(p[1])] for p in points]\n",
    "\n",
    "        # If no pattern matches, try to extract numbers and pair them\n",
    "        numbers = re.findall(r\"-?\\d+\\.?\\d*\", text)\n",
    "        if len(numbers) >= 2 and len(numbers) % 2 == 0:\n",
    "            points = []\n",
    "            for i in range(0, len(numbers), 2):\n",
    "                points.append([float(numbers[i]), float(numbers[i + 1])])\n",
    "            return points\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError, IndexError) as e:\n",
    "        print(f\"Failed to parse trace: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "class ApiModel():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        base_url:str,\n",
    "        user_prompt: str,\n",
    "        system_prompt: str,\n",
    "        normalized_coordinates: bool = True,\n",
    "        request_delay: float = 0.5,\n",
    "        retry_delay: float = 2,\n",
    "        max_retries: int = 3,\n",
    "        max_tokens: int = 5000,\n",
    "        temperature: float = 1.0,\n",
    "    ):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.user_prompt = user_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.normalized_coordinates = normalized_coordinates\n",
    "        self.request_delay = request_delay\n",
    "        self.retry_delay = retry_delay\n",
    "        self.max_retries = max_retries\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Configure OpenAI client\n",
    "        api_key = os.environ.get(\"MODEL_API_KEY\")\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "\n",
    "    def process_sample(self, sample: Dict, embodiment: str) -> Dict[str, Any]:\n",
    "\n",
    "        # Extract fields\n",
    "        sample_id = sample[\"sample_id\"]\n",
    "        category = sample[\"category\"]\n",
    "        image = sample[\"image\"]\n",
    "        task = sample[\"task\"]\n",
    "\n",
    "        # Format prompt\n",
    "        prompt = self.user_prompt.format(task=task, embodiment=embodiment)\n",
    "\n",
    "        # Encode image to base64\n",
    "        image_b64 = encode_image_to_base64(image)\n",
    "\n",
    "        # Prepare message content\n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"},\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]\n",
    "\n",
    "        # Limit request rate\n",
    "        time.sleep(self.request_delay)\n",
    "\n",
    "        # Make API request with retries\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": content},\n",
    "                    ],\n",
    "                    max_tokens=self.max_tokens,\n",
    "                    temperature=self.temperature,\n",
    "                )\n",
    "\n",
    "                # Extract the trace\n",
    "                response_text = response.choices[0].message.content\n",
    "                predicted_trace = parse_trace(response_text)\n",
    "\n",
    "                # Unnormalize coordinates\n",
    "                if self.normalized_coordinates:\n",
    "                    width, height = image.size\n",
    "                    predicted_trace = [\n",
    "                        [int(x * width), int(y * height)] for x, y in predicted_trace\n",
    "                    ]\n",
    "\n",
    "                # Extract reasoning if available\n",
    "                if hasattr(response.choices[0].message, \"reasoning\"):\n",
    "                    reasoning_text = response.choices[0].message.reasoning\n",
    "                else:\n",
    "                    reasoning_text = \"\"\n",
    "\n",
    "                return {\n",
    "                    \"sample_id\": sample_id,\n",
    "                    \"embodiment\": embodiment,\n",
    "                    \"category\": category,\n",
    "                    \"raw_response\": response_text,\n",
    "                    \"reasoning\": reasoning_text,\n",
    "                    \"prediction\": predicted_trace,\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}. Retrying in {self.retry_delay} seconds...\")\n",
    "                    time.sleep(self.retry_delay)\n",
    "                else:\n",
    "                    print(f\"Failed after {self.max_retries} attempts: {e}\")\n",
    "                    return {\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"embodiment\": embodiment,\n",
    "                        \"category\": category,\n",
    "                        \"raw_response\": \"\",\n",
    "                        \"reasoning\": \"\",\n",
    "                        \"prediction\": [],\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fab594",
   "metadata": {},
   "source": [
    "### Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e585e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = ApiModel(model_name, base_url, user_prompt, system_prompt)\n",
    "\n",
    "# Iterate over dataset\n",
    "results = []\n",
    "dataset = dataset[\"validation\"]\n",
    "for i, sample in tqdm(enumerate(dataset), desc=f\"Inference with {model_name}\", total=len(dataset)):\n",
    "\n",
    "    # Iterate over emodiments of a sample\n",
    "    embodiments = sample[\"embodiments\"]\n",
    "    for embodiment in embodiments:\n",
    "        result = model.process_sample(sample, embodiment)\n",
    "        results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "model_safe_name = model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_path = f\"./{model_safe_name}_validation_{timestamp}.tsv\"\n",
    "results_df.to_csv(\n",
    "    results_path,\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa039e",
   "metadata": {},
   "source": [
    "### Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc93a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = ApiModel(model_name, base_url, user_prompt, system_prompt)\n",
    "\n",
    "# Iterate over dataset\n",
    "results = []\n",
    "dataset = dataset[\"test\"]\n",
    "for i, sample in tqdm(enumerate(dataset), desc=f\"Inference with {model_name}\", total=len(dataset)):\n",
    "\n",
    "    # Iterate over emodiments of a sample\n",
    "    embodiments = sample[\"embodiments\"]\n",
    "    for embodiment in embodiments:\n",
    "        result = model.process_sample(sample, embodiment)\n",
    "        results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "model_safe_name = model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_path = f\"./{model_safe_name}_test_{timestamp}.tsv\"\n",
    "results_df.to_csv(\n",
    "    results_path,\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71d128",
   "metadata": {},
   "source": [
    "## Calculate Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import functools\n",
    "import json\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "from skimage.draw import line_aa\n",
    "from skimage.draw import line as sk_line\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee017b4e",
   "metadata": {},
   "source": [
    "### Define Penalty Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "PENALTY_SCORES_PATH = \"./category_penalty.tsv\"\n",
    "M2F_CONFIG_PATH = \"./mask2former_config.json\"\n",
    "\n",
    "@functools.lru_cache(maxsize=4)\n",
    "def create_penalty_lookup(embodiment: str) -> Dict[int, float]:\n",
    "    \"\"\"Creates a direct mapping from a category ID (`label_id`) to its penalty factor.\"\"\"\n",
    "\n",
    "    # Load fixed penalty values\n",
    "    penalty_values_df = pd.read_csv(PENALTY_SCORES_PATH, sep=\"\\t\")\n",
    "\n",
    "    # Load Mask2Former mapping from IDs to labels\n",
    "    with open(M2F_CONFIG_PATH, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    id2label = {int(k): v for k, v in config[\"id2label\"].items()}\n",
    "\n",
    "    label_id_to_penalty = {}\n",
    "    for label_id, category_name in id2label.items():\n",
    "        \n",
    "        # Look up the penalty value\n",
    "        row = penalty_values_df[\n",
    "            penalty_values_df[\"category\"] == category_name\n",
    "        ]\n",
    "        penalty_value = float(row.iloc[0][embodiment]) * 0.8  # Adjust scale\n",
    "        label_id_to_penalty[label_id] = penalty_value\n",
    "\n",
    "    return label_id_to_penalty\n",
    "\n",
    "def rasterize_gt_trace(\n",
    "    gt_trace: List[List[float]], height: int, width: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Converts a line trace into a dense array of pixel coordinates.\"\"\"\n",
    "\n",
    "    gt_trace_np = np.array(gt_trace)\n",
    "    gt_line_pixels = []\n",
    "    if len(gt_trace_np) > 1:\n",
    "        for i in range(len(gt_trace_np) - 1):\n",
    "            p1, p2 = gt_trace_np[i], gt_trace_np[i + 1]\n",
    "            r0, c0, r1, c1 = (\n",
    "                int(round(p1[1])),\n",
    "                int(round(p1[0])),\n",
    "                int(round(p2[1])),\n",
    "                int(round(p2[0])),\n",
    "            )\n",
    "            rr, cc, _ = line_aa(r0, c0, r1, c1)\n",
    "            valid = (rr >= 0) & (rr < height) & (cc >= 0) & (cc < width)\n",
    "            gt_line_pixels.extend(zip(rr[valid], cc[valid]))\n",
    "    elif len(gt_trace_np) == 1:\n",
    "        r, c = int(round(gt_trace_np[0][1])), int(round(gt_trace_np[0][0]))\n",
    "        if 0 <= r < height and 0 <= c < width:\n",
    "            gt_line_pixels.append((r, c))\n",
    "\n",
    "    return np.array(gt_line_pixels)\n",
    "\n",
    "def create_penalty_mask(\n",
    "    segmentation_mask: np.ndarray,\n",
    "    gt_trace: List[List[float]],\n",
    "    embodiment: str,\n",
    "    distance_threshold: float = 35,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    # Initialize mask with default no penalty\n",
    "    height, width = segmentation_mask.shape\n",
    "    penalty_mask = np.full((height, width), 0, dtype=float)\n",
    "\n",
    "    # Create a KDTree from ground truth pixels for efficient distance queries\n",
    "    gt_line_pixels = rasterize_gt_trace(gt_trace, height, width)\n",
    "    gt_tree = KDTree(gt_line_pixels)\n",
    "\n",
    "    # Create a more efficient lookup for segment info and penalty values\n",
    "    label_id_to_penalty = create_penalty_lookup(embodiment)\n",
    "\n",
    "    # Get label IDs for all pixels\n",
    "    all_label_ids = segmentation_mask.ravel()\n",
    "\n",
    "    # Identify pixels that belong to undesired segments\n",
    "    undesired_mask = np.isin(all_label_ids, list(label_id_to_penalty.keys()))\n",
    "    undesired_indices = np.where(undesired_mask)[0]\n",
    "    if undesired_indices.size == 0:\n",
    "        return penalty_mask\n",
    "\n",
    "    # Map indices to coordinates\n",
    "    rows, cols = np.unravel_index(undesired_indices, (height, width))\n",
    "    undesired_coords = np.vstack((rows, cols)).T\n",
    "\n",
    "    # Perform a single batch query for distances for all undesired pixels\n",
    "    distances, _ = gt_tree.query(undesired_coords)\n",
    "\n",
    "    # Filter for pixels that are beyond the distance threshold\n",
    "    coords_to_penalize = undesired_coords[distances > distance_threshold]\n",
    "\n",
    "    if coords_to_penalize.size > 0:\n",
    "        # Apply penalties\n",
    "        rows_pen, cols_pen = coords_to_penalize[:, 0], coords_to_penalize[:, 1]\n",
    "        label_ids_to_penalize = segmentation_mask[rows_pen, cols_pen]\n",
    "        penalties = np.vectorize(label_id_to_penalty.get)(label_ids_to_penalize)\n",
    "        penalty_mask[rows_pen, cols_pen] = penalties\n",
    "\n",
    "    return penalty_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c97ff",
   "metadata": {},
   "source": [
    "### Define Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_SCORE_THRESHOLD = 3234.75\n",
    "\n",
    "def resample_to_match_length(\n",
    "    trace_1: np.ndarray, trace_2: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    if len(trace_1) == 0 or len(trace_2) == 0:\n",
    "        raise ValueError(\"One of the traces is empty\")\n",
    "    if len(trace_1) == len(trace_2):\n",
    "        return trace_1, trace_2\n",
    "    elif len(trace_1) > len(trace_2):\n",
    "        longer, shorter = (trace_1, trace_2)\n",
    "    else:\n",
    "        shorter, longer = (trace_1, trace_2)\n",
    "    if len(shorter) == 1:\n",
    "        return shorter * len(longer), longer\n",
    "\n",
    "    # Parameterize shorter trajectory by cumulative distance\n",
    "    dists = np.cumsum(\n",
    "        [0]\n",
    "        + [np.linalg.norm(shorter[i] - shorter[i - 1]) for i in range(1, len(shorter))]\n",
    "    )\n",
    "    dists = dists / dists[-1]  # Normalize to [0,1]\n",
    "\n",
    "    # Create new parameter values matching longer trajectory length\n",
    "    new_params = np.linspace(0, 1, len(longer))\n",
    "\n",
    "    # Interpolate x and y coordinates separately\n",
    "    new_x = np.interp(new_params, dists, shorter[:, 0])\n",
    "    new_y = np.interp(new_params, dists, shorter[:, 1])\n",
    "    shorter = np.column_stack([new_x, new_y])\n",
    "\n",
    "    if len(trace_1) > len(trace_2):\n",
    "        return longer, shorter\n",
    "    else:\n",
    "        return shorter, longer\n",
    "\n",
    "def calculate_semantic_penalty(\n",
    "    prediction: np.ndarray, penalty_mask: np.ndarray\n",
    ") -> List[float]:\n",
    "\n",
    "    penalties = []\n",
    "    for i in range(len(prediction) - 1):\n",
    "        x1, y1 = int(round(prediction[i][0])), int(round(prediction[i][1]))\n",
    "        x2, y2 = int(round(prediction[i + 1][0])), int(round(prediction[i + 1][1]))\n",
    "\n",
    "        # Use scikit-image's optimized line drawing algorithm\n",
    "        rr, cc = sk_line(y1, x1, y2, x2)\n",
    "\n",
    "        # Access mask using (y, x) coordinates\n",
    "        height, width = penalty_mask.shape\n",
    "        valid_indices = (rr >= 0) & (rr < height) & (cc >= 0) & (cc < width)\n",
    "        penalties.extend(penalty_mask[rr[valid_indices], cc[valid_indices]].tolist())\n",
    "\n",
    "    return np.mean(penalties)\n",
    "\n",
    "def calculate_fde(prediction: np.ndarray, ground_truth: np.ndarray):\n",
    "\n",
    "    return np.linalg.norm(prediction[-1] - ground_truth[-1])\n",
    "\n",
    "def calculate_dtw(prediction: np.ndarray, ground_truth: np.ndarray):\n",
    "\n",
    "    # Create cost matrix\n",
    "    n, m = len(prediction), len(ground_truth)\n",
    "    cost_matrix = np.full((n + 1, m + 1), np.inf)\n",
    "    cost_matrix[0, 0] = 0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            euclidean_distance = np.linalg.norm(prediction[i - 1] - ground_truth[j - 1])\n",
    "\n",
    "            # Find the minimum from the three possible previous cells\n",
    "            min_prev_cost = min(\n",
    "                cost_matrix[i - 1, j],  # Insertion\n",
    "                cost_matrix[i, j - 1],  # Deletion\n",
    "                cost_matrix[i - 1, j - 1],  # Match\n",
    "            )\n",
    "\n",
    "            cost_matrix[i, j] = euclidean_distance + min_prev_cost\n",
    "\n",
    "    return cost_matrix[n, m]\n",
    "\n",
    "def normalize_score(score: float) -> float:\n",
    "\n",
    "    # Normalize score so that a perferct score is at 100 and a score worse than the avg. performance of predicting a vertical line through the center is < 0\n",
    "    return (BAD_SCORE_THRESHOLD - score) / BAD_SCORE_THRESHOLD * 100\n",
    "\n",
    "def score(\n",
    "    prediction: List[List[float]],\n",
    "    ground_truths: List[List[List[float]]],\n",
    "    segmentation_mask: np.ndarray,\n",
    "    embodiment: str,\n",
    "):\n",
    "    \n",
    "    # Iterate over all ground-truths\n",
    "    scores = []\n",
    "    for ground_truth in ground_truths:\n",
    "        \n",
    "        # Create penalty mask\n",
    "        penalty_mask = create_penalty_mask(segmentation_mask, ground_truth, embodiment)\n",
    "\n",
    "        # Convert to NumPy\n",
    "        prediction, ground_truth = np.array(prediction), np.array(ground_truth)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if len(prediction) != len(ground_truth):\n",
    "            prediction, ground_truth = resample_to_match_length(prediction, ground_truth)\n",
    "\n",
    "        # Calculate score function\n",
    "        sem_penalty = calculate_semantic_penalty(prediction, penalty_mask)\n",
    "        fde = calculate_fde(prediction, ground_truth)\n",
    "        dtw = calculate_dtw(prediction, ground_truth)\n",
    "        scores.append(dtw + fde + sem_penalty)\n",
    "        \n",
    "    # Select the best score\n",
    "    score = min(scores)\n",
    "\n",
    "    # Normalize\n",
    "    return normalize_score(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97156b1",
   "metadata": {},
   "source": [
    "### Parallelized Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_worker(results_path, dataset_id, split_name):\n",
    "\n",
    "    global _results_df, _get_sample\n",
    "    \n",
    "    # Load data\n",
    "    _results_df = pd.read_csv(results_path, sep=\"\\t\")\n",
    "    data_split = load_dataset(dataset_id)[split_name]\n",
    "\n",
    "    # Build lookup index for efficient sample retrieval\n",
    "    id_to_index = {sample_id: i for i, sample_id in enumerate(data_split[\"sample_id\"])}\n",
    "    \n",
    "    def get_sample(sample_id):\n",
    "        idx = id_to_index[sample_id]\n",
    "        return data_split[idx]\n",
    "    \n",
    "    _get_sample = get_sample\n",
    "\n",
    "\n",
    "def _score_chunk(indices: List[int]) -> List[Tuple[int, float]]:\n",
    "\n",
    "    results = []\n",
    "    for idx in indices:\n",
    "        row = _results_df.loc[idx]\n",
    "        \n",
    "        # Extract necessary data for scoring\n",
    "        sample = _get_sample(row[\"sample_id\"])\n",
    "        embodiment = row[\"embodiment\"]\n",
    "        try:\n",
    "            prediction = json.loads(row[\"prediction\"])\n",
    "        except (json.JSONDecodeError, TypeError, ValueError):  # Skip invalid predictions\n",
    "            results.append((idx, np.nan))\n",
    "            continue\n",
    "        if len(prediction) == 0:  # Skip invalid predictions\n",
    "            results.append((idx, np.nan))\n",
    "            continue\n",
    "        ground_truths = sample[\"ground_truth\"][row[\"embodiment\"]]\n",
    "        if ground_truths is None:  # Check that ground-truth is not hidden as it is for the test split\n",
    "            raise ValueError(f\"The sample {sample} has hidden ground-truths\")\n",
    "        segmentation_mask = np.array(sample[\"segmentation_mask\"])\n",
    "\n",
    "        # Calculate score\n",
    "        s = score(prediction, ground_truths, segmentation_mask, embodiment)\n",
    "        results.append((idx, s))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def score_predictions_parallel(results_path, dataset_id, split_name, num_processes=4):\n",
    "\n",
    "    # Load results file\n",
    "    results_df = pd.read_csv(results_path, sep='\\t')\n",
    "\n",
    "    # Split work into chunks\n",
    "    total_rows = len(results_df)\n",
    "    chunk_size = (total_rows + num_processes - 1) // num_processes  # Ceiling division\n",
    "    indices_chunks = [\n",
    "        list(range(i, min(i + chunk_size, total_rows)))\n",
    "        for i in range(0, total_rows, chunk_size)\n",
    "    ]\n",
    "        \n",
    "    # Process chunks in parallel\n",
    "    scored_df = results_df.copy()\n",
    "    scored_df[\"score\"] = np.nan\n",
    "    with multiprocessing.Pool(\n",
    "        processes=num_processes,\n",
    "        initializer=_initialize_worker,\n",
    "        initargs=(\n",
    "            results_path,\n",
    "            dataset_id,\n",
    "            split_name,\n",
    "        ),\n",
    "    ) as pool:\n",
    "        with tqdm(total=total_rows, desc=\"Scoring predictions\") as pbar:\n",
    "            for chunk_results in pool.imap_unordered(_score_chunk, indices_chunks):\n",
    "                for idx, s in chunk_results:\n",
    "                    scored_df.at[idx, \"score\"] = s\n",
    "                pbar.update(len(chunk_results))\n",
    "    \n",
    "    return scored_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434c807",
   "metadata": {},
   "source": [
    "### Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Select results file manually\n",
    "# results_path = \"./path/to/validation/results.tsv\"\n",
    "\n",
    "# Calculate score in parallel\n",
    "scored_df = score_predictions_parallel(results_path, \"leggedrobotics/navitrace\", \"validation\", num_processes=4)\n",
    "\n",
    "# Save results with scores\n",
    "score_path = Path(results_path)\n",
    "score_path = score_path.parent / f\"score_{score_path.name}\"\n",
    "scored_df.to_csv(score_path, sep=\"\\t\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33c69f",
   "metadata": {},
   "source": [
    "### Visualize Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Score\n",
    "scored_df = pd.read_csv(score_path, sep=\"\\t\")\n",
    "total_score = scored_df['score'].mean()\n",
    "\n",
    "# Score per embodiment\n",
    "score_per_embodiment = scored_df.groupby('embodiment')['score'].mean()\n",
    "\n",
    "# Score per category\n",
    "scored_df['category'] = scored_df['category'].apply(ast.literal_eval)\n",
    "exploded_df = scored_df.explode('category')\n",
    "score_per_category = exploded_df.groupby('category')['score'].mean()\n",
    "\n",
    "# Number of scores that are np.nan\n",
    "nan_count = scored_df['score'].isna().sum()\n",
    "\n",
    "# Print score summary\n",
    "width = 32\n",
    "title_len = 15\n",
    "print(\" \" * ((width - title_len) // 2) + \"ðŸ¥‡ \\033[1mSCORE SUMMARY\\033[0m\")\n",
    "print(width * \"â”\")\n",
    "print(f\"\\033[1mTotal Score\\033[0m           : {total_score:>8.2f}\")\n",
    "print(f\"\\033[1mInvalid Predictions\\033[0m   : {nan_count:>8}\")\n",
    "print(f\"\\033[1mScore per Embodiment\\033[0m\")\n",
    "for embodiment, score in score_per_embodiment.items():\n",
    "    print(f\"- {embodiment:<20}: {score:>8.2f}\")\n",
    "print(f\"\\033[1mScore per Category\\033[0m\")\n",
    "for category, mean_score in score_per_category.items():\n",
    "    print(f\"- {category:<20}: {mean_score:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303dd6f",
   "metadata": {},
   "source": [
    "### Test Split\n",
    "\n",
    "We hide the ground-truths for the test split to allow for a fair leaderboard and to prevent model training.\n",
    "But you can use this [Hugging Face Space](https://huggingface.co/spaces/leggedrobotics/navitrace_leaderboard) to calculate your test scores and optionally submit your model to the leaderboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
